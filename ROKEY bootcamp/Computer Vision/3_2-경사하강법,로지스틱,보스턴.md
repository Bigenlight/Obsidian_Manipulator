#컴퓨터비전 
[[컴퓨터 비전]]
2024-08-16

![[3차시_03.pdf]]
# 경사하강법
[[3_1-기초수학,선형회귀,최소제곱법,평균제곱오차,와인실습]]
이제 임의의 선을 평군제곱오차로 오차를 구했으면 이를 다시 조정해야함.

![[Pasted image 20240816225243.png|400]]
a1, a2, a3에 대한 오차의 기울기를 구함(편미분).
오차가 점차 감소하는데, 오차의 기울기가 점차 감소하는 쪽으로 진행해야 함.
궁극적으로 기울기가 0이면 오차가 더 이상 줄어들지 않는 지점에 온거임.
그러면 거기가 오차가 최소인 직선인 곳.

이때 얼만큼 이동할지는 학습률로 정함. 이는 최적의 값을 잘 정하는 수밖에 없음.

평균제곱오차 공식:
![[Pasted image 20240816225623.png|200]]
직선 공식 집어 넣음
![[Pasted image 20240816225649.png|200]]
편미분
![[Pasted image 20240816225743.png|300]]

?? 잠만 오차면.... 그냥 주어지는 것 아닌가? 어떻게 기울기를 구하지? 이전 오차에 대한 차이가 기울기가... 음.
-> 뭔 개소리냐, 그러면 지금까지 수학을 할 때 이전과 다음 값 없이 기울기를 어케 구했냐, 미분하잖아 그래서! 편미분하면 오차에 대한 기울기를 구할 수 있잖음! -> 아! 그러네 컴퓨터라서 왜 미분을 하면 안된다고 생각했지?

오차 기울기가 음수면 증가하고, 기울기가 양수면 감소함.

실습
```python
import numpy as np
import matplotlib.pyplot as plt

# 공부 시간 x와 성적 y의 넘파이 배열을 생성
x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])

# 데이터의 분포를 그래프로
plt.scatter(x, y)
plt.show()

a = 0
b = 0
lr = 0.03 # 학습률

# 몇 번 반복될지 설정
epochs = 2001
n = len(x) # x 값이 총 몇 개인지
  
# 경사 하강법을 시작
for i in range(epochs):
    y_pred = a * x + b  # 예측 값을 구하는 식
    error = y - y_pred  # 실제 값과 비교한 오차를 error로
    
    a_diff = -(2/n) * sum(x * error)  # 오차 함수를 a로 편미분한 값
    b_diff = -(2/n) * sum(error)      # 오차 함수를 b로 편미분한 값 

    a = a - lr * a_diff  # 학습률을 곱해 기존의 a 값을 업데이트
    b = b - lr * b_diff  # 학습률을 곱해 기존의 b 값을 업데이트  

    if i % 100 == 0:  # 100번 반복될 때마다 출력
        print("epoch=%.f, 기울기=%.04f, 절편=%.04f" % (i, a, b))
  
# 앞서 구한 최종 a 값을 기울기, b 값을 y 절편에 대입해 그래프 생성
y_pred = a * x + b 

# 그래프
plt.scatter(x, y)
plt.plot(x, y_pred, 'r')
plt.show()
```
![[Pasted image 20240816230640.png|300]]


#### 다중 선형 회귀
![[Pasted image 20240816231016.png|200]]
여러개의 독립변수도 가능
![[Pasted image 20240816231111.png|400]]

```python
import numpy as np
import matplotlib.pyplot as plt

# 데이터 준비: x1은 공부 시간, x2는 과외 시간, y는 성적
x1 = np.array([2, 4, 6, 8])
x2 = np.array([0, 4, 2, 3])
y = np.array([81, 93, 91, 97])

# 데이터의 분포를 3D 그래프로 시각화
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(x1, x2, y)
plt.show()

# 기울기와 절편 초기화
a1 = 0
a2 = 0
b = 0

# 학습률 설정
lr = 0.01

# 반복 횟수 설정
epochs = 2001

# 데이터 포인트의 수
n = len(x1)

# 경사 하강법 시작
for i in range(epochs):
    y_pred = a1 * x1 + a2 * x2 + b  # 예측값 계산
    error = y - y_pred  # 실제값과 예측값의 차이
    
    # 오차 함수의 기울기 계산
    a1_diff = (2/n) * sum(-x1 * error)  # a1에 대한 편미분
    a2_diff = (2/n) * sum(-x2 * error)  # a2에 대한 편미분
    b_diff = (2/n) * sum(-error)        # b에 대한 편미분
    
    # 학습률을 곱한 값으로 파라미터 업데이트
    a1 = a1 - lr * a1_diff
    a2 = a2 - lr * a2_diff
    b = b - lr * b_diff
    
    # 100번 반복할 때마다 결과 출력
    if i % 100 == 0:
        print(f"epoch={i}, 기울기1={a1:.4f}, 기울기2={a2:.4f}, 절편={b:.4f}")

# 실제 성적과 예측된 성적을 출력
print("실제 성적: ", y)
print("예측 성적: ", y_pred)

```

# 텐서플로우
텐서플로우로 선형회귀
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense # 딥러닝 때 레이러 용도
import numpy as np
import matplotlib.pyplot as plt

# 데이터
x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])

# 모델 설정
model = Sequential()
model.add(Dense(1, input_dim=1, activation='linear'))

# 모델 컴파일
model.compile(optimizer='sgd', loss='mse')
  
# 모델 학습
model.fit(x, y, epochs=200)
# 그래프 출력
plt.scatter(x, y)
plt.plot(x, model.predict(x), 'r')  # 예측 결과를 그래프로 나타낸다
plt.show()

# 임의의 시간을 집어넣어 점수를 예측하는 모델을 테스트.
hour = 7
prediction = model.predict(np.array([hour]))
print("%.f시간을 공부할 경우의 예상 점수는 %.02f점입니다." % (hour, prediction))
```


![[3차시_04.pdf]]
# 로지스틱
직선으로 해결 안되는 것도 있음
![[Pasted image 20240816233139.png|400]]
종속변수(결과)가 0과 1 처럼 불연속적인 경우

선형 회귀는 평균 제곱 오차 함수로 구한다면,
로지스틱 회귀에서는 ==교차 엔트로피 함수==
###### 시그모이드
![[Pasted image 20240816233307.png|200]]
a값에 따른 변화
![[Pasted image 20240816233333.png|300]]
b는 자체 이동
![[Pasted image 20240816233431.png|300]]



![[3차시_05.pdf]]
## 보스턴 주택 가격
[Google Colab](https://colab.research.google.com/drive/1L7JwgKDPMC6Jr0DfQrz7Xk12iCNvA41G?hl=ko#scrollTo=mBwq6H1K2_Fi)