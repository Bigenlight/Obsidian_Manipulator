#컴퓨터비전 
[[Ai,컴퓨터 비전]]
2024-08-19

![[1.Intro_for_AI-0819_-_배포안.pdf]]

인공지능 수학 배우면 다 어려움, 수학 책 펼치면 닫고 싶음. 하지만 그 구간을 넘기면 재미있어짐.

CNN, RNN, GAN등이 다 LLM안에 있음

결국 y = f(x) + b의 정확한 y 값을 도출하기 위한 작업이 AI. 어마어마한 양을 트레이닝하여, 다양한 x인풋에 따른 유사 y 값을 예측하는 것. AI를 통해 그동안 만들지 못하는 함수를 만드는 것, 인공지능은 특정한 패턴을 찾는데 유리하기 때문에.

데이터를 많이 학습시키는 것은 많이 부담 됨. 그래서 주로 단순화나 표본을 뽑음. 그런 방식으로 learning rate을 줄임.

![[Pasted image 20240819122401.png]]
여기서 우리는 딥러닝에서도 CNN를 배울 예정.

딥러닝은 그렇게 어렵지 않다.
basic algebra + probability + python
1년 이하의 교육 과정으로 충분히 이해 적용 가능

텐서플로우 위주로 가르치긴하는데
욜로 쪽은 또 토치라서 토치도 좀 알려드림

무엇보다 재미있다.

y hat은 예측한 y값
![[Pasted image 20240819141348.png|300]]

![[Pasted image 20240819141851.png|300]]

==RMSE: Root Mean Sqare Error==
평균제곱오차에 루트 씌움.
![[Pasted image 20240819141057.png]]
![[Pasted image 20240819141118.png]]

### Gradient Descent : 경사하강법
공식 설명
![[Pasted image 20240819152142.png]]
기울기 알파가 학습률
![[Pasted image 20240819152436.png|270]]

## Backpropagation : 역전파
[오차역전파 (Backprogation)의 개념을 쉽게 이해해 봅시다 - YouTube](https://www.youtube.com/watch?v=1Q_etC_GHHk&t=358s)
영상을 한번 보는게 이해에 도움 됨.

![[Pasted image 20240819203808.png|400]]
이전에 경사하강법에서 했던 선형 방정식에서 gradient(오차 기울기)를 구함

나중에 또 배우니 가볍게 볼 것
![[Pasted image 20240819204145.png|300]]
통계에서 h는 가설의 약자임.

Chani rule: 체인 룰
![[Pasted image 20240819204334.png]]
g랑 f가 각각 은닉층 레이어라고 보면 됨.
x가 들어오고 f가 나감.
이때 두 레이어의 변화율을 곱하면(미분에 미분을 하면) 총 변화율과 같다.

원래 시그모이드 함수를 사용했는데 이게 문제가 있음. 위에 처럼 미분에 미분하다 보면 너무 0에 근접함. 그래서 요즘은 Relu 씀.

그래서 원래는 이렇게 단방향인데
![[Pasted image 20240819205246.png|400]]
역전파에서는 역방향으로 gradiet 값을 또 집어넣음. 앞에 있던 값을 또 뒤로 보냄.
![[Pasted image 20240819205311.png|500]]
뒤로 지나갈때마다 새로 편미분되어 바뀜.

예시
![[Pasted image 20240819205602.png|600]]

이번에는 다른 그림으로 역전파 설명
원래 제곱오차를 나타내는 다이어그램이 있을 때
![[Pasted image 20240819210112.png]]
x = 1, y =2, w = 1일 때
역전파는
![[Pasted image 20240819210301.png]]
최종 목표는 결국 아래를 구하는 것.
![[Pasted image 20240819213329.png|100]]
이전에 했던 Local gradient를 
![[Pasted image 20240819213817.png|100]]
인 Global gradient에 곱하기만 하면 나온다.

Local gradient는 정방향(forward pass) 계산 중에 쉽게 계산할 수 있기 때문에, 복잡한 층에서도 gradient을 구하기가 쉬워진다.

Forward pass는 입력에 따른 최종 결과를 계산하는 과정이고, 역전파는 forward pass로 계산된 출력과 실제 값 사이의 오차를 바탕으로 가중치와 편향을 업데이트하는 ==학습==의 과정이다. 업데이트는 옵티마이저로.

![[Pasted image 20240819213329.png|90]]
각각의 변수에 대한 gradient 값이 크면 클수록 최종값에 영향을 많이 미친다는 것임.
이를 통해 w를 조정할 수 있음.

![[_최종__AI개론교안_4차시_A._모두의딥러닝.pdf]]
[[_최종__AI개론교안_4차시_B._머신러닝교과서.pdf]]