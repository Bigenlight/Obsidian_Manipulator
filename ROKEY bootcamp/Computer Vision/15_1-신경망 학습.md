#컴퓨터비전 
[[Ai,컴퓨터 비전]]
last modification: 2024-09-03

# 4장 - 학습
[Google Colab](https://colab.research.google.com/drive/1i0TRsW7-eJJeoKojiWIQmze8dR47LvBQ#scrollTo=uY09nxMXJx4_)
이론 있음.
### 손실함수
- **오차를 구함**
- loss function / cost function / object fucntion
- 신경망 학습에서 사용하는 지표 - 학습이 잘 진행되고 있는지 판단
- 평균 제곱 오차(선형)와 교차 엔트로피(비선형) 오차 주로 사용

###### 평균제곱오차
$$MSE=\frac{1}{2}{\sum_{i=1}^n (Y_{i} - \bar Y_{i})^2}$$
전체 오차의 합
선형일때(항등 함수) 적용.
```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```
###### 교차 엔트로피 오차
소프트맥스에서 사용
$$CEE={- \sum_{i=1}^n    t_{i}  \log(y_{i})}$$
이게 복잡해 보여도 생각보다 계산이 간단. t가 결과 라벨인데 이게 정답말고 다 0임. 그래서 정답 라벨 하나의 값만 보면 됨. 이때 결과 출력에 0~1에서 나오는데, 이게 작으면 작을수록 음의 무한대로 감. 0.6이면 -0.51 되고, 0.1이면 -2.3 됨. 그리고 앞에 - 있으니 붙이고 오차 구하는겨.(아마도?) 
```python
def cross_entropy_error(y, t):
    delta = 1e-7                          # 0일때 -무한대가 되지 않기 위해 작은 값을 더함
    return -np.sum(t * np.log(y + delta))
```
###### 미니 배치 학습
대충만 봄
데이터 많아지면 모든 곳의 손실 함수를 계산하면 너무 시간이 오래 걸림. 그래서 걍 몇개 뽑아서 그것들만 계산하여 전체 추측하는 것.

### 수치 미분
$$\dfrac{df(x)}{dx}=\lim_{h\to 0} \frac{f(x + h) - f(x-h)}{2h} $$
![[Pasted image 20240903120957.png|500]]
수학에서는 h를 0으로 수렴 시키는데 PC에서는 불가능하니 h값을 매우 작게 설정.
```python
def numerical_diff(f, x):
    h = 10e-4
    return (f(x + h) - f(x - h)) / (2 * h)
```

### 기울기

#### 경사하강법
$$ x_{0} = x_{0} - \eta \dfrac{\partial f}{\partial x_{0}}$$

$$ x_{1} = x_{1} - \eta \dfrac{\partial f}{\partial x_{1}}$$
learning rate는 hyper parameter임
==hyper parameter은 사람이 튜닝 해야== 하는 파라미터라는 뜻


2x3신경망의 weight, 그리고 구한 Loss 변화를 weight 변화로 나눔.
$$ W = \left(\begin{array}{rrr}

w_{11}&w_{12}&w_{13}\\

w_{21}&w_{22}&w_{23}

\end{array}\right)$$
$$ \dfrac{\partial L}{\partial W} = \left(\begin{array}{rrr}

\dfrac{\partial L}{\partial w_{11}}&\dfrac{\partial L}{\partial w_{12}}&\dfrac{\partial L}{\partial w_{13}}\\

\dfrac{\partial L}{\partial w_{21}}&\dfrac{\partial L}{\partial w_{22}}&\dfrac{\partial L}{\partial w_{23}}

\end{array}\right)$$
그것을 learning rate로 곱하고 원래 weight에 뺌으로써 변화 적용. 새 weight를 가짐.
$$ {W} = \left(\begin{array}{rrr}

w_{11} - a \dfrac{\partial L}{\partial w_{11}}&w_{12} - a \dfrac{\partial L}{\partial w_{12}}&w_{13} - a \dfrac{\partial L}{\partial w_{13}}\\

w_{21} - a \dfrac{\partial L}{\partial w_{21}}&w_{22} - a \dfrac{\partial L}{\partial w_{22}}&w_{23} - a \dfrac{\partial L}{\partial w_{23}}

\end{array}\right)$$

> [!np.exp()]
> np.exp( ) 함수는 곧 e^( ) 이라는 뜻

## 학습 알고리즘 구현
수치 미분과 공식 미분이 있는데
수치 미분은 계산하는데 시간이 너무 오래 걸려 안 씀. (이게 진짜 수학적으로 미분 하는 방식)
공식 미분이 이전에 배운 방식임. <- 역전파 방식