#컴퓨터비전 
[[컴퓨터 비전]]
2024-08-16

![[3차시_01.pdf]]
[Google Colab](https://colab.research.google.com/drive/1L7JwgKDPMC6Jr0DfQrz7Xk12iCNvA41G?hl=ko#scrollTo=mBwq6H1K2_Fi)
# 딥러닝을 위한 기초 수학
## 기초 수학
딥러닝에서도 코드를 아는 것보다 원리를 이해하는 것이 중요함.

이차함수
![[Pasted image 20240816222145.png|400]]

미분
![[Pasted image 20240816222215.png|500]]
![[Pasted image 20240816222226.png|200]]
![[Pasted image 20240816222256.png|180]]

###### 편미분
![[Pasted image 20240816222327.png|250]]

##### 시그모이드
![[Pasted image 20240816222427.png|200]]
![[Pasted image 20240816222445.png|400]]
0과 1로 판단할 때 유용
## ==선형회귀==
데이터만 제대로 있다면 새로운 값이 가져올 결과를 예측할 수 있음.
데이터의 경향성을 나타내는 1차 방정식을 만듬. 

하나의 독립변수로 종속 변수를 만들 수 있으면 단순 선형 회귀.
여러개의 독립변수로 종속 변수를 만들면 다중 선형 회귀.


공식

### 최소제곱법
y = a * x + b
![[Pasted image 20240816223259.png|300]]
![[Pasted image 20240816223327.png|300]]
![[Pasted image 20240816223359.png|400]]
```python
import numpy as np

x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])

# x 평균값
mx = np.mean(x)
# y 평균값
my = np.mean(y)
  
print("x의 평균값: ", mx)
print("y의 평균값: ", my)
  
# 기울기 공식 분모 부분
divisor = sum([(i - mx) ** 2 for i in x])
# 기울기 공식 분자 부분
def top(x, mx, y, my):
    d = 0
    for i in range(len(x)):
        d += (x[i] - mx) * (y[i] - my)
    return d
  
dividend = top(x, mx, y, my)

print("분모: ", divisor)
print("분자: ", dividend)
  
# 기울기 a
a = dividend / divisor
# y 절편 b
b = my - (mx * a)

print("기울기 a = ", a)
print("y 절편 b = ", b)
```

### 평균 제곱오차
최소제곱법은 좋은 방법이지만 여러개의 독립변수에(입력변수) 대응하기 무리.

일단 선 하나를 만들고, 오차를 점차 줄이는 방법을 택할 때.
이때 오차를 구할 때 사용 되는 방법이 ==평균 제곱 오차==

![[Pasted image 20240816224027.png|300]]
yi가 각 실제 데이터 값, yi(모자)는 예측선으로 만들어낸 값.
이들의 오차의 평균

이제 이거를 줄이는 방법은 경사 하강법 같은 것을 이용.

코드 실습
```python
import numpy as np

# 임의 선
fake_a = 3
fake_b = 76

# 배열 생성
x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])

# y = ax + b에 가상의 a 값과 b 값을 대입한 결과를 출력
def predict(x):
    return fake_a * x + fake_b  

# 예측 값이 들어갈 빈 리스트
predict_result = []
  
# 모든 x 값을 한 번씩 대입해 predict_result 리스트를 완성
for i in range(len(x)):
    predict_result.append(predict(x[i]))
    print("공부시간=%.f, 실제점수=%.f, 예측점수=%.f" % (x[i], y[i], predict(x[i])))
  

# 평균 제곱 오차 함수를 각 y 값에 대입해 최종 값을 구하는 함수
n = len(x)
  
def mse(y, y_pred):
    return (1/n) * sum((y - y_pred) ** 2)
  
# 평균 제곱 오차 값
print("평균 제곱 오차: " + str(mse(y, predict_result)))
```


![[3차시_02.pdf]]
## 와인 실습
[Site Unreachable](https://colab.research.google.com/drive/1L7JwgKDPMC6Jr0DfQrz7Xk12iCNvA41G?hl=ko#scrollTo=Xr60ZvJVIUT8)


