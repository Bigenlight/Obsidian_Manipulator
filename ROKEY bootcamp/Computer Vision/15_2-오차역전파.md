#컴퓨터비전 
[[Ai,컴퓨터 비전]]
last modification: 2024-09-03

# 5장
[Google Colab](https://colab.research.google.com/drive/1H4Nr84rgc0pKU1W1q94BsSKEE1AFF6Kj#scrollTo=Mvzhe91SC4WQ) <- 코드 다시 한번 보면 될 듯
## 오차역전파 = Back Propogation

![[Pasted image 20240903152412.png|500]]

==덧셈 노드의 역전파는 입력값을 그대로== 뒤로 흘려보냄
![[Pasted image 20240904112702.png|500]]
왜냐? x + y르 x든  y든 편미분을 하면 1이 나오기 때문. 그래서 걍 1 곱하면 됨.
![[Pasted image 20240904114149.png|400]]

==곱셉 노드는 다른 입력을 곱하고== 뒤로 보냄
![[Pasted image 20240904114205.png|500]]
왜냐? x * y를 편미분하면 다른 쪽만 남기 때문, 그래서 그냥 반대 곱하고 뒤로 넘기면 됨.
![[Pasted image 20240904114444.png]]
참고로 만약 소비세가 1만큼 증가하면 그만큼 최종 가격이 200만큼 증가한다는 얘기.

근데 왜 정확하게 계산하는데 1 만큼의 역전파 입력이 들어오는거임?
그냥 예시인가?
만약 최종값이 1만큼 증가했고, 그리고 오차도 1만큼 증가했으면. 오차 편미분 / 최종 편미분 = 1이다. 즉, 위와 같은 것이다. 그러면 가중치를 업데이트하고 싶으면 결국 오차 편미분 / 가중치 편미분이니, 사광 같은 경우 1 / 2.2 = 5/11을. 사과의 개수는 1 / 110를, 소비세는 1.1 / 200에 learning rate를 곱하여 업데이트하면 되는 것인가? 

아무튼 이렇게 gradient 값을 구하면 어떤 가중치가 많은 영향을 미치는지 알 수 있음.


![[Pasted image 20240903153116.png]]
이런거 채워 넣는 문제 나올 수 있을 듯.

###### 답: 
 ![[Pasted image 20240924020316.png]]

##### Relu 역전파는 
만약 9보다 크면 그대로 뒤로 전달
작으면 0
![[Pasted image 20240925154954.png]]

##### 시그모이드 역전파는
![[Pasted image 20240925155643.png|400]]
이거임, 외우셈
참고로 정방향은 이거
![[Pasted image 20240925155907.png|200]]
